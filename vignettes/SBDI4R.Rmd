---
title: "Introduction to SBDI4R"
author: "Alejandro Ruete and Debora Arlt"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: TRUE
vignette: >
  %\VignetteIndexEntry{Introduction to SBDI4R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(width = 120)
```  

## Using SBDI4R

Lets assume you have already installed the package as shown in the main site \url{https://biodiversitydata-se.github.io/SBDI4R}. 

The SBDI4R package must be loaded for each new R session:

```{r}
library(SBDI4R)
```  
However, the options you stored in .Rprofile if you did it so, will load automatically with the package. Then, check that we have some additional packages that we'll use in the examples, and install them if necessary.
```{r message=FALSE}
to_install <- c("ape", "dplyr", "ggplot2", "jpeg", "maps", "mapdata",
                "maptools", "phytools", "sp", "rgeos", "tidyr", "vegan")
to_install <- to_install[!sapply(to_install, requireNamespace, quietly=TRUE)]
if(length(to_install)>0)
    install.packages(to_install, repos="http://cran.us.r-project.org")
```

#SHOULD WE BRING THE EXAMPLES FROM THE MAIN PAGE HERE?
### Example 2: Area report: what listed species exist in a given area?

Vector spatial layers (eg. Polygons) can be imported  in a number of different ways. 
Bioatlas' APIs take as search input polygons in the s.k. WKT (Well Known Text \url{https://www.geoapi.org/3.0/javadoc/org/opengis/referencing/doc-files/WKT.html}). 
So the first step is to load a vector layer and transform it into a WKT string. 
First download a .zip file with different delimitations for Sweden \url{https://www.scb.se/hitta-statistik/regional-statistik-och-kartor/regionala-indelningar/digitala-granser}
and move it somewhere you like in your computer. We recommend you move it into your 
working directory (\code{getwd()}). Extract the .zip file named KommunSweref99.zip.
<!-- We use the ALA4R's caching mechanism here, but you could equally download this file directly. -->

```{r}
library(rgdal)
# shape <- readOGR(dsn=file.path("your/path/to/file", "Kommun_Sweref99TM_region.shp"))
```
This will only work when you set a valid filepath, and will create an object of 
class SpatialPolygon. You could instead use the data we kindly provided in this 
package \code{data("swe")} 

```{r}
shape <- swe$Municipalities
## extract just the Municipality of Örebro
shape <- shape[shape$KnNamn=="Örebro", ]
```

We could create the WKT string using the `rgeos` library:
```{r eval=FALSE}
library(rgeos)
wkt <- writeWKT(shape)
```

Unfortunately, in this instance this gives a WKT string that is too long and won't
be accepted by the web service. Also, the shapefile we just got is projected in 
the coordinate system SWEREF99 TM, and the web service only accepts coordinates in 
a geodesic coordinate system WGS84. Instead, let's construct the WKT string directly, 
which gives us a little more control over its format:
```{r}
shape <- sp::spTransform(shape, CRSobj = CRS("+init=epsg:4326")) ## the magic number for WGS84
lonlat <- shape@polygons[[1]]@Polygons[[1]]@coords ## extract the polygon coordinates
## extract the convex hull of the polygon to reduce the length of the WKT string
temp <- chull(lonlat)
lonlat <- lonlat[c(temp, temp[1]), ]
## create WKT string
## first join each lon-lat coordinate pair
temp <- apply(lonlat, 1, function(z) paste(z, collapse=" "))
## now build the WKT string
wkt <- paste("POLYGON((", paste(temp, collapse=","), "))", sep="")
```

Now extract the species list in this polygon:
```{r eval=FALSE}
specieslist(wkt=wkt) %>%
    dplyr::arrange(desc(occurrenceCount)) %>%
    dplyr::select(speciesName, species, family, occurrenceCount) %>%
    head(10)
```

```{r echo=FALSE}
tryCatch({
specieslist(wkt=wkt) %>%
    dplyr::arrange(desc(occurrenceCount)) %>%
    dplyr::select(speciesName, species, family, occurrenceCount) %>%
    head(10)
}, error = function(e) { print(e$message)})
```

#FROM HERE ON NOT ADAPTED TO SBDI
### Example 4: Community composition and turnover

```{r message=FALSE}
library(vegan)
```

Define our area of interest as a transect running westwards from the Sydney region, and download the occurrences of legumes (Fabaceae; a large family of flowering plants) in this area:
```{r eval=FALSE}
wkt <- "POLYGON((152.5 -35,152.5 -32,140 -32,140 -35,152.5 -35))"
## define some environmental layers of interest [see ala_fields()]
env_layers <- c("Precipitation - annual","Temperature - annual max mean")
## Download the data.  We use the `occurrences()` function, adding environmental
##   data via the 'extra' parameter. Note that method="offline" supports
##   unlimited download size and more fields (but is slower).
## You should adjust the `download_reason_id` to match your purposes if using
##   this function for your own analyses; see `ala_reasons()`
x <- occurrences(taxon="family:Fabaceae", wkt=wkt, qa="none",
                 download_reason_id="testing", extra=env_layers,
                 email="test@test.org")
```

Convert this to a sites-by-species data.frame:

```{r include=FALSE}
## load data from a local copy so that vignette building doesn't require downloading a big chunk of data and slow sites-by-species processing
## this file generated by running the above unevaluated code blocks, then
## saveRDS(xgridded, file="vignette_fabaceae.rds")
xgridded <- readRDS("vignette_fabaceae.rds")
sppcols <- setdiff(names(xgridded), c("longitude", "latitude", "precipitationAnnual", "temperatureAnnualMaxMean", "richness"))
```

```{r eval=FALSE}
xgridded <- x$data %>%
    ## discard genus- and higher-level records
    dplyr::filter(rank %in%
                  c("species", "subspecies", "variety", "form", "cultivar")) %>%
    ## bin into 0.5-degree bins
    mutate(longitude=round(longitude*2)/2, latitude=round(latitude*2)/2) %>%
    ## average environmental vars within each bin
    group_by(longitude,latitude) %>%
    mutate(precipitationAnnual=mean(precipitationAnnual, na.rm=TRUE),
           temperatureAnnualMaxMean=mean(temperatureAnnualMaxMean, na.rm=TRUE)) %>%
    ## subset to vars of interest
    dplyr::select(longitude, latitude, scientificName, precipitationAnnual,
                  temperatureAnnualMaxMean) %>%
    ## take one row per cell per species (presence)
    distinct() %>%
    ## calculate species richness
    mutate(richness=n()) %>%
    ## convert to wide format (sites by species)
    mutate(present=1) %>%
    do(tidyr::spread(data=., key=scientificName, value=present, fill=0)) %>%
    ungroup()
## where a species was not present, it will have NA: convert these to 0
sppcols <- setdiff(names(xgridded),
                   c("longitude", "latitude", "precipitationAnnual", "temperatureAnnualMaxMean",
                     "richness"))
xgridded <- xgridded %>% mutate_at(sppcols, function(z) ifelse(is.na(z), 0, z))
```

The end result:
```{r}
xgridded
```

Now we can start to examine the patterns in the data. Let's plot richness as a function of longitude:
```{r warning=FALSE}
library(ggplot2)
ggplot(xgridded, aes(longitude, richness)) + geom_point() + theme_bw()
```

The number of species is highest at the eastern end of the transect (the Sydney/Blue Mountains area). This probably reflects both higher species richness as well as greater sampling effort in this area compared to the western end of the transect.

Species richness as a function of environment:
```{r warning=FALSE}
ggplot(xgridded, aes(temperatureAnnualMaxMean, precipitationAnnual, colour=richness)) +
    scale_colour_distiller(palette="Spectral") + geom_point(size=8) + theme_bw()
```

Higher species richness in cooler, wetter areas (i.e. the Blue Mountains).

How does the community composition change along the transect? Use clustering:

```{r fig.width=6, fig.height=6}
## Bray-Curtis dissimilarity
D <- vegdist(xgridded[, sppcols], "bray")
## UPGMA clustering
cl <- hclust(D, method="ave")
## plot the dendrogram
plot(cl)
## extract group labels at the 20-group level
grp <- cutree(cl, 20)
## coalesce small (outlier) groups into a single catch-all group
sing <- which(table(grp)<5)
grp[grp %in% sing] <- 21 ## put these in a new combined group
grp <- sapply(grp, function(z)which(unique(grp)==z)) ## renumber groups
xgridded$grp <- as.factor(grp)
## plot
## colours for clusters
thiscol <- c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", "#8c564b", "#e377c2",
             "#7f7f7f", "#bcbd22", "#17becf")
ggplot(xgridded, aes(longitude, latitude, colour=grp)) + geom_point(size=5) +
    scale_colour_manual(values=thiscol) + theme_bw()
## or a slightly nicer map plot
library(maps)
library(mapdata)
map("worldHires", "Australia", xlim=c(105, 155), ylim=c(-45, -10), col="gray90", fill=TRUE)
with(xgridded, points(longitude, latitude, pch=21, col=thiscol[grp], bg=thiscol[grp], cex=0.75))
```

### Example 5: Search and download species occurrence records with images

We can download images from the ALA's image service using the id field, which is the ALA's 
occurrence record identifier. We would first search occurrences() using facets to filter the records we want, in this case, magpie occurrences with an associated image and an open licence.

```{r eval=FALSE}
magpie_occs <- ALA4R::occurrences(taxon="taxon_name:\"Gymnorhina tibicen\"",
                            fq=c("multimedia:Image","license:\"CC0\""))
```

The top 5 records:

```{r eval=FALSE}
magpie_occs_top5 <- magpie_occs$data %>% 
    dplyr::select(id,basisOfRecord,dataResourceName,state,licence,eventDate) %>% 
    head(5)
```


```{r eval=FALSE, echo=FALSE}
tryCatch({
  magpie_occs <- ALA4R::occurrences(taxon="taxon_name:\"Gymnorhina tibicen\"",
                            fq=c("multimedia:Image","license:\"CC0\""), 
                            email = "test@ala-test.org", download_reason_id = "testing")
  # retain 5
  magpie_occs_top5 <- magpie_occs$data %>% dplyr::arrange(desc(eventDate)) %>% 
    dplyr::mutate(occId=paste0(substring(id,0,10),"...")) %>%
    dplyr::select(id,occId,basisOfRecord,dataResourceName,state,licence,eventDate) %>% 
    head(5)
  
  # display
  if (!is.null(dim(magpie_occs_top5))) { magpie_occs_top5 %>% 
      dplyr::select(occId,basisOfRecord,dataResourceName,state,licence,eventDate)}   
},warning = function(w) {print(w$message)}
 ,error = function(e) { print(e$message)})
```  

We then call the occurrence_images() function to query the image service using the occurrence ids.

```{r eval=FALSE}
magpie_occ_images <- ALA4R::occurrence_images(magpie_occs_top5 %>% 
    dplyr::pull(id), download=FALSE)
```

```{r echo=FALSE,warning=FALSE}
tryCatch({
  magpie_occ_images <- ALA4R::occurrence_images(magpie_occs_top5 %>% dplyr::pull(id), download=FALSE)
  
  if (!is.null(dim(magpie_occ_images))) {
    magpie_occ_images %>% dplyr::mutate(occurrenceID=paste0(substring(occurrenceID,0,10),"..."), imageID = paste0(substring(imageIdentifier,0,10),"...")) %>%
      dplyr::select(occurrenceID, imageID, format, fileSize, width, height)
  }
},error = function(e) { print(e$message)})
```

Set the download flag in the occurrence_images() function and optionally specify a file path to download the images to local disk. The image download is rate limited to 1 image/second to minimise server load.

```{r eval=FALSE}
magpie_occ_images <- ALA4R::occurrence_images(magpie_occs_top5 %>% 
    dplyr::pull(id), download=TRUE, download_path = "my/local/directory")
```

### Example 6: Retrieve assertion information for datasets
Compare data quality metrics for data resources
```{r}
dr <- data_resources(druid = c('dr1411','dr90','dr361'), extra = 'assertions')
```

```{r eval = FALSE}
# View names of all columns returned
names(dr)
```

Extract the assertion metrics from the dataset
```{r warning = FALSE}
library(tidyr)
# match cols against known assertions
assertions <- ala_fields('assertions')
assert_match <- names(dr)[names(dr) %in% assertions$name]
dr_assert <- dr %>% select(uid, all_of(assert_match)) %>%
  pivot_longer(-uid, names_to = "assertion", values_to = "count") %>%
  mutate(count = as.integer(trimws(as.character(count)))) %>%
  # build axes labels with readable assertions
  mutate(assertion_label = tolower(gsub('([A-Z])','\n\\1',assertion)))
```

Plot the assertions
```{r warning=FALSE}
library(ggplot2)
ggplot(dr_assert) + geom_bar(aes(x = assertion_label, y = count,
                                 fill = uid),
                             stat = "identity", position = "dodge",
                             width = 2/3) +
  theme(axis.text.x = element_text(size = 7)) + 
  labs(x = "Assertions")
```

### Example 6: Retrieve assertion information for datasets
Compare data quality metrics for data resources
```{r}
dr <- data_resources(druid = c('dr1411','dr90','dr361'), extra = 'assertions')
```

```{r eval = FALSE}
# View names of all columns returned
names(dr)
```

Extract the assertion metrics from the dataset
```{r warning = FALSE}
library(tidyr)
# match cols against known assertions
assertions <- ala_fields('assertions')
assert_match <- names(dr)[names(dr) %in% assertions$name]
dr_assert <- dr %>% select(uid, all_of(assert_match)) %>%
  pivot_longer(-uid, names_to = "assertion", values_to = "count") %>%
  mutate(count = as.integer(trimws(as.character(count)))) %>%
  # build axes labels with readable assertions
  mutate(assertion_label = tolower(gsub('([A-Z])','\n\\1',assertion)))
```

Plot the assertions
```{r warning=FALSE}
library(ggplot2)
ggplot(dr_assert) + geom_bar(aes(x = assertion_label, y = count,
                                 fill = uid),
                             stat = "identity", position = "dodge",
                             width = 2/3) +
  theme(axis.text.x = element_text(size = 7)) + 
  labs(x = "Assertions")
```